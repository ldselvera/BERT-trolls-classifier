{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'GeForce GTX 1080 Ti'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/luisselvera/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/luisselvera/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/luisselvera/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/luisselvera/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/luisselvera/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/luisselvera/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "#Trolls comment data\n",
    "Non_Russian=pd.read_csv(\"data/csv-zusammenfuehren.de_3mjg6fs7.csv\", error_bad_lines=False)\n",
    "\n",
    "#Non trolls comment data\n",
    "Russian = pd.read_csv(\"data/comments.csv\")\n",
    "\n",
    "#Get only the comments\n",
    "Non_Russian = Non_Russian[[\"body\"]]\n",
    "Russian = Russian[[\"body\"]]\n",
    "\n",
    "#Dropping rows with N/A\n",
    "Non_Russian.dropna(inplace=True)\n",
    "Russian.dropna(inplace=True)\n",
    "\n",
    "num_samples = Russian.shape[0]\n",
    "Non_Russian = Non_Russian.sample(num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A hard look at training and tactics\" = They will be sent more $$$ for \"training\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>They deserve all of the hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I guess that's what they mean when say \"I don't see color\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It's never too late for them, It's never too cruel or brutal for them. He will still probably get away with this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://petitions.whitehouse.gov//petition/petition-create-nationwide-elected-and-publicly-reviewed-police-oversight-agency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>By submitting to an independent, non-profit community, the authors volunteered on a Good Samaritan basis to spread wokeness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sorry, but if you bothered to read the article below the video you could've gotten the point</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>That is not a personal blog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Only 125? Why not 10?\\r\\nIslamic state has captured half Syria, they are close to Baghdad and keep advancing while all western leaders do is simply taking some minor measures. Smh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What? Any other wild guesses about my private life?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                  body\n",
       "0  A hard look at training and tactics\" = They will be sent more $$$ for \"training\"                                                                                                   \n",
       "1  They deserve all of the hate                                                                                                                                                       \n",
       "2  I guess that's what they mean when say \"I don't see color\"                                                                                                                         \n",
       "3  It's never too late for them, It's never too cruel or brutal for them. He will still probably get away with this                                                                   \n",
       "4  https://petitions.whitehouse.gov//petition/petition-create-nationwide-elected-and-publicly-reviewed-police-oversight-agency                                                        \n",
       "5  By submitting to an independent, non-profit community, the authors volunteered on a Good Samaritan basis to spread wokeness                                                        \n",
       "6  Sorry, but if you bothered to read the article below the video you could've gotten the point                                                                                       \n",
       "7  That is not a personal blog                                                                                                                                                        \n",
       "8  Only 125? Why not 10?\\r\\nIslamic state has captured half Syria, they are close to Baghdad and keep advancing while all western leaders do is simply taking some minor measures. Smh\n",
       "9  What? Any other wild guesses about my private life?                                                                                                                                "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Russian[['body']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32317</th>\n",
       "      <td>Fine the way it is.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23630</th>\n",
       "      <td>With Vsync 45fps is the same as 30fps AFAIK.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10610</th>\n",
       "      <td>Having other good players is great but Chubb is our workhorse. He has the build to take the hits that come from 20 carries a game.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28958</th>\n",
       "      <td>They changed it to say \"right wing\" authoritarianism.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34131</th>\n",
       "      <td>John Wall averages 2008 Chris Paul numbers as he leads the Wizards to a top 2 seed and the Finals as MVP, sweeping LeBron and the Cavs in the conference finals but not before LeBron, at the line with a chance to win game 4 in the series and prevent a sweep, is iced by Wall and subsequently misses his free throws.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35811</th>\n",
       "      <td>Manen to the rescue!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>&amp;gt;Randall Cobb\\n&amp;gt;24\\n&amp;gt;old age\\nkek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34179</th>\n",
       "      <td>It's a rodent that died tried to get into the shake. RIP, Rat.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13946</th>\n",
       "      <td>Ozai, as in this guy: http://static.comicvine.com/uploads/original/11114/111147344/3976312-4231998611-69941.gif\\nI tried to make a hunter that was the complete opposite of me, so I made a huge strong guy who uses lances (not anymore though) and has a beard and stuff.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44477</th>\n",
       "      <td>Its performance is unmatched and the camera is, without exaggeration, amazing. Also it's Google's new main focus now the nexus project has been shelved so expect it to be supported to all hell</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                             body\n",
       "32317  Fine the way it is.                                                                                                                                                                                                                                                                                                       \n",
       "23630  With Vsync 45fps is the same as 30fps AFAIK.                                                                                                                                                                                                                                                                              \n",
       "10610  Having other good players is great but Chubb is our workhorse. He has the build to take the hits that come from 20 carries a game.                                                                                                                                                                                        \n",
       "28958  They changed it to say \"right wing\" authoritarianism.                                                                                                                                                                                                                                                                     \n",
       "34131  John Wall averages 2008 Chris Paul numbers as he leads the Wizards to a top 2 seed and the Finals as MVP, sweeping LeBron and the Cavs in the conference finals but not before LeBron, at the line with a chance to win game 4 in the series and prevent a sweep, is iced by Wall and subsequently misses his free throws.\n",
       "35811  Manen to the rescue!                                                                                                                                                                                                                                                                                                      \n",
       "234    &gt;Randall Cobb\\n&gt;24\\n&gt;old age\\nkek                                                                                                                                                                                                                                                                                \n",
       "34179  It's a rodent that died tried to get into the shake. RIP, Rat.                                                                                                                                                                                                                                                            \n",
       "13946  Ozai, as in this guy: http://static.comicvine.com/uploads/original/11114/111147344/3976312-4231998611-69941.gif\\nI tried to make a hunter that was the complete opposite of me, so I made a huge strong guy who uses lances (not anymore though) and has a beard and stuff.                                               \n",
       "44477  Its performance is unmatched and the camera is, without exaggeration, amazing. Also it's Google's new main focus now the nexus project has been shelved so expect it to be supported to all hell                                                                                                                          "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Non_Russian.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentence and label lists\n",
    "Russian_sentences = Russian.body.values\n",
    "Non_Russian_sentences = Non_Russian.body.values\n",
    "\n",
    "# Special initial and EOS (end of sentence) tokens\n",
    "Russian_sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in Russian_sentences]\n",
    "Non_Russian_sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in Non_Russian_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize the first sentence:\n",
      "['[CLS]', 'a', 'hard', 'look', 'at', 'training', 'and', 'tactics', '\"', '=', 'they', 'will', 'be', 'sent', 'more', '$', '$', '$', 'for', '\"', 'training', '\"', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "Russian_tokenized_texts = [tokenizer.tokenize(sent) for sent in Russian_sentences]\n",
    "Non_Russian_tokenized_texts = [tokenizer.tokenize(sent) for sent in Non_Russian_sentences]\n",
    "\n",
    "Russian_tokenized_texts = [x for x in Russian_tokenized_texts if len(x) < 128]\n",
    "Non_Russian_tokenized_texts = [x for x in Non_Russian_tokenized_texts if len(x) < 128]\n",
    "\n",
    "tokenized_texts = Non_Russian_tokenized_texts + Russian_tokenized_texts\n",
    "\n",
    "print (\"Tokenize the first sentence:\")\n",
    "print (tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding Labels\n",
    "Non_Russian_labels = [0 for x in range(len(Non_Russian_tokenized_texts))]\n",
    "Russian_labels = [1 for x in range(len(Russian_tokenized_texts))]\n",
    "\n",
    "labels = Non_Russian_labels + Russian_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max sequence length.\n",
    "MAX_LEN = 128\n",
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks = []\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our data into train and validation sets for training\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=2018, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids, random_state=2018, test_size=0.1)\n",
    "\n",
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterator of data\n",
    "batch_size = 32\n",
    "epochs = 4\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained BERT model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "model.cuda()\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "# Hyperparemeter information\n",
    "optimizer = BertAdam(optimizer_grouped_parameters, lr=2e-5, warmup=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy of predictions\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "t = [] \n",
    "train_loss_set = []\n",
    "\n",
    "# trange is a tqdm wrapper around the normal python range\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        train_loss_set.append(loss.item())    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(train_loss_set)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
